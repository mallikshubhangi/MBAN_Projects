{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all relevant packages and importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame Checks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows, excluding the 'id' column as 'id' column is a unique identifier for each recipe\n",
    "duplicate_rows = df.duplicated(subset=df.columns.difference(['id'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning is a critical step in the data preprocessing phase. It involves handling missing values, removing duplicates, and addressing inconsistencies or errors present in the dataset. By ensuring the cleanliness and reliability of the data, we significantly reduce the likelihood of biased or inaccurate analysis.\n",
    "Data cleaning plays a pivotal role in achieving \"tidy data,\" which is essential for conducting further operations and analyses effectively. In this section, we will meticulously analyze the data types of variables and correct them if necessary. Additionally, we will identify and handle null values in the dataset.\n",
    "By adhering to robust data cleaning practices, we ensure that the dataset is in optimal condition for subsequent analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the copy() function to make a copy of the DataFrame\n",
    "df_test = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain descriptive statistics by Region using groupby()\n",
    "df_test.groupby('Region').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing Region data with \"Other\" and compare statistics\n",
    "df_test['Region'] = df_test['Region'].fillna('Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('Region').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_2 = df.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers\n",
    "\n",
    "Now that we have cleaned up the structural issues in our DataFrame, we should check for outliers. The simple way to do this is by using a BoxPlot. We can use the seaborn boxplot() function to identify outliers.\n",
    "\n",
    "There are more sophisticated techniques, for example calculating the z-score of each value, however since we have not yet done this in class, we will use a simple approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data = df, y='Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find index values of observations above 1000\n",
    "drop_price_outliers = df[df['Price']>1000].index\n",
    "drop_price_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(drop_price_outliers, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now finished the data cleaning steps! Our final DataFrame has 890 entries we can use for analysis. To summarize how we approached cleaning the data:\n",
    "\n",
    "We dropped rows that had multiple missing values, which would cause challenges for imputation\n",
    "We reviewed missing values in detail, and compared various approaches to imputing data. For the Region, we assessed whether we should impute with \"Other\" or the mode - since the mode resulted in value changes, we decided to go with \"Other\"\n",
    "For the Product, we compared the approach of \"Other\", the mode, or imputing with random product assignments. Neither approach was great, and since we had only 10% of missing data points we decided that as long as we fill in missing Price and Units Sold data, we can drop the missing data from this column\n",
    "For Price, we compared imputation by the column median, median by Product, or median by Region. Option 1 and 3 yielded the same results, and Option 2 created bias - as such, we decided to use the column median\n",
    "For Units Sold, we compared imputation by the column median, median by Product, or median by Region. All 3 options yielded similar results, so for simplicity we used the column median\n",
    "We changed the data type of the Sale Date column to datetime (this was an optional step for this assignment)\n",
    "We used a boxplot to identify outliers in the Price and Units Sold column, and removed outliers\n",
    "We dropped all duplicate entries\n",
    "Now, let's move on to Data Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration\n",
    "\n",
    "The purpose of the data exploration stage is to identify trends and patterns in data. First, we should explore each variable by itself, and then we should explore variables in pairs or combinations. Our objective for the Data Exploration stage is to identify clear patterns or recommendations we can make to the business.\n",
    "\n",
    "Let's start with univariate analysis.\n",
    "\n",
    "For the continuous variables (Price and Units Sold), we can use histograms to understand spread. For categorical variables, we can use counts and bar charts to see which groups are most popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "print(stats.skew(df['Price']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['Units Sold'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on skewness and kurtosis, we use z score to remove outliers\n",
    "if abs(s) > 1:\n",
    "        df[column] = df[column].apply(lambda x: np.log(x + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = zscore(df[numerical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(np.abs(z_scores) < threshold).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After we removing the outlier, we can see that the value of skewness and Kurtosis had changed significantly.\n",
    "for column in numerical_columns:\n",
    "    skewness_values = []\n",
    "    s = skew(df[column])\n",
    "    k = kurtosis(df[column])\n",
    "    print(f'{column}: skewness = {s}, kurtosis = {k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Region')['Units Sold'].count().sort_values(ascending=False).plot(kind='bar', figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_by_region['sum'].sort_values(ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data = df, x='Price', y='Units Sold')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering\n",
    "\n",
    "Generate new features from the data you have. This could include:\n",
    "binning (i.e., translate a continuous variable into groups like 5-10, 15-20, etc.)\n",
    "dummy variables (use one hot encoding, or pandas get_dummies() function to convert categorical variables to dummies)\n",
    "define new metrics (e.g., multiply columns together or create custom categories based on multiple variables)\n",
    "Check the relationship of engineered features to the output variable, using the methods outlined above"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula: Precision = True Positives / (True Positives + False Positives)\n",
    "When to use precision:\n",
    "When the cost of false positives is high. For example, in medical diagnoses, you want to be very certain before classifying someone as having a disease, as a false positive could lead to unnecessary treatments or stress.\n",
    "When you want to prioritize the accuracy of positive predictions over other metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula: Recall = True Positives / (True Positives + False Negatives)\n",
    "When to use recall:\n",
    "When the cost of false negatives is high. For instance, in spam email detection, it's important to avoid missing spam emails even if it means some legitimate emails are classified as spam (false positives).\n",
    "When the goal is to capture as many positive instances as possible, even if it results in more false positives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
